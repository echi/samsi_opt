---
title: "Lab 2"
date: "August 11, 2016"
output: pdf_document
---
**Part 1.** We will investigate algorithms for iteratively solving the lasso problem or 1-norm regularized least squares problem. We first set some notation. Let $x_i \in \mathbb{R}^p, y_i \in \mathbb{R}$ for $i=1, \ldots, n$. Let $X \in \mathbb{R}^{n \times p}$ denote the matrix with $x_i$ as its $i$th row.
Recall that the penalized negative log-likelihood $\ell(\beta)$ can be written as follows:
$$
\ell(\beta) = \frac{1}{2} \lVert y - Xb \rvert_2^2 + \lambda \lVert \beta \rVert_1
$$

1. Write the gradient and Hessian of $f(\beta) = \frac{1}{2} \lVert y - Xb \rvert_2^2$.

\vspace{2cm}

2. What is the computational complexity for a calculating the gradient and Hessian of $f(\beta)$?

\vspace{2cm}

3. Under what condition is $\ell(\beta)$ strictly convex?

\vspace{2cm}

4. Prove that $f(\beta)$ is $L$-Lipschitz differentiable with $L = \lVert X \rVert_{\text{op}}^2$.

\vspace{2cm}

5. Prove that for all $\lambda \geq \lVert X^Ty \rVert_\infty$, the lasso solution is the all zeros vector.

\vspace{2cm}

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(myopt)
```

**Part 2.** Coordinate Descent, Proximal Gradient Descent, and ADMM

**Step 1:** Write a function "lasso_cd" The function should terminate either when a maximum number of iterations has been taken or if the relative change in the objective function has fallen below a tolerance

$$
\frac{\lvert f(x_k) - f(x_{k-1})\rvert}{\lvert f(x_{k-1}) \rvert + 1)} < \text{tolerance}
$$

```{r, echo=TRUE}
#' Lasso (Coordinate Descent)
#'
#' \code{lasso_cd} solves the lasso problem using coordinate descent.
#'
#' @param y Response variable
#' @param X design matrix
#' @param beta0 initial guess of regression parameter
#' @param lambda regularization parameter
#' @param max_iter maximum number of iterations
#' @param tol convergence tolerance
#' @export
lasso_cd <- function(y, X, beta0, lambda, max_iter=1e2, tol=1e-3) {

}
```

- The final iterate value
- The objective function values

**Step 2:** Write a function "lasso_pgd." 

```{r, echo=TRUE}
#' Lasso (Proximal Gradient Descent)
#'
#' \code{lasso_pgd} solves the lasso problem using proximal gradient descent with a fixed step size
#'
#' @param y Response variable
#' @param X design matrix
#' @param beta0 initial guess of regression parameter
#' @param lambda regularization parameter
#' @param t step-size
#' @param max_iter maximum number of iterations
#' @param tol convergence tolerance
#' @export
lasso_pgd <- function(y, X, beta0, lambda, t, max_iter=1e2, tol=1e-3) {

}
```
Your function should return

- The final iterate value
- The objective function values

\newpage

**Step 3:** Write a function "lasso_admm."

```{r, echo=TRUE}
#' Lasso (ADMM)
#'
#' \code{lasso_admm} solves the lasso problem using ADMM
#'
#' @param y Response variable
#' @param X design matrix
#' @param beta0 initial guess of regression parameter
#' @param lambda regularization parameter
#' @param rho parameter
#' @param max_iter maximum number of iterations
#' @param tol convergence tolerance
#' @export
lasso_admm <- function(y, X, beta0, lambda, rho=1, max_iter=1e2, tol=1e-3) {

}
```

Your function should return

- The final iterate value
- The (primal) objective function values

**Step 4:** Perform lasso regression with different values of $\lambda$ on the following data example $(y,X)$. Use your answers to Part 1 to choose an appropriate fixed step-size for the proximal gradient algorithm. Plot the objective function values $\ell(\beta_k)$ versus the iteration $k$ for all three methods. How does the lasso do at recovering the true sparse model?

```{r, echo=TRUE}
set.seed(12345)
n <- 100
p <- 2000

X <- matrix(rnorm(n*p),n,p)
beta0 <- matrix(0,p,1)
beta0[1:5] <- runif(5)

y <- X%*%beta0 + rnorm(n)
```

```{r, echo=FALSE, fig.width=6, fig.height=3}
x <- 1:10
y <- 1:10
plot(x, y, type='n')
```

\newpage

**Part 5.** If you have additional time,

- Try different values of $\rho$ for ADMM. What is the effective on the convergence speed?
- Try adding in backtracking into the proximal gradient code.
- Implement FISTA, compare the run times with regular proximal gradient descent.

